#
# kubestrap
#   prepare a newly-provisioned node for service before returning to terraform
#
# desc:
#   - called as provisioner on all plexhoc nodes
#   - works on kubemasters, kubeslaves, and standalone plexhocs
#   - waits for cloudinit completion
#   - if has kube-apiserver process:
#       - copy kubeconfig to controller (todo: masters > 1)
#       - install and wait for CNI
#   - else if has kubelet process:
#       - wait for kubectl node-ready status
#   - else noop (ie, non-kubenode)
#
---

- name: prepare_node
  hosts: '{{nodename}}'
  vars_files:
    - vars/networks.yml
    - vars/pysetup.yml
    - vars/kube.yml
  tasks:

    # allow kubeadm init/join to finish
    - name: wait_for_cloudinit_completion
      command: timeout {{init_join_timeout}} cloud-init status --wait
      changed_when: false

    # use presence of kubernetes processes to determine node type
    - name: determine_node_type
      block:

        - name: determine_if_kubenode
          command: pgrep -x kubelet
          register: r_kubelet
          failed_when: false
          changed_when: false

        - name: determine_if_kubectl
          command: pgrep -x kube-apiserver
          register: r_kubectl
          failed_when: false
          changed_when: false
          when: r_kubelet.rc == 0

        - name: set_kfacts
          set_fact:
            is_knode: '{{r_kubelet.rc == 0}}'
            is_kctl:  '{{r_kubectl.rc == 0}}'
            is_kwrk:  '{{r_kubelet.rc == 0 and r_kubectl.rc != 0}}'
            is_knone: '{{r_kubelet.rc != 0 and r_kubectl.rc != 0}}'
          changed_when: false

    - name: copy_master_kubeconfig_to_controller
      synchronize:
        mode: pull
        perms: false
        dest: "{{kubeconfig}}"
        src: /etc/kubernetes/admin.conf
        use_ssh_args: true
      become: true
      when: is_kctl

    - name: kubemaster_tasks
      when: is_kctl
      become: false
      delegate_to: localhost
      vars:
        ansible_python_interpreter: "{{py3bin_setup}}"
      block:

        # make symlinks to ~setup/.kube/*.yml work for setup group
        - name: make_kubeconfig_group_readwrite
          file:
            mode: '0660' # keep as string to avoid bugs
            path: "{{kubeconfig}}"

        - name: helm_install_cilium_repo
          kubernetes.core.helm_repository:
            #atomic: true
            name: cilium
            repo_url: https://helm.cilium.io/
            kubeconfig: "{{kubeconfig}}"

        - name: helm_install_cilium
          kubernetes.core.helm:
            #atomic: true
            kubeconfig: "{{kubeconfig}}"
            release_name: cilium
            chart_ref: cilium/cilium
            release_namespace: kube-system
            release_values:

              # in future, we might use clustermesh, so these should be unique
              cluster:
                name: "{{plexname}}"
                id: "{{octet3}}"

              # explicitly configure access to kube control plane via local
              k8sServiceHost: '{{ansible_default_ipv4.address}}'
              k8sServicePort: 6443

              # we have used skipPhases kube-proxy in ClusterConfiguration, so
              # network doesn't work until cilium is installed.
              #
              kubeProxyReplacement: true

              # this can be inferred from podSubnet, but the auto-detection is
              # inappropriate for advanced scenarios like multiple subnets.
              # since we already have the information, we'll just use it here.
              #
              ipam: {operator: {clusterPoolIPv4PodCIDRList: "{{podnet}}"}}

              # even though we run everything on one physical system, in
              # theory we can survive failed cilium pod upgrades better if we
              # have a standby replica.  todo: this should be researched more.
              # UPDATE: using 1 for now, let's make it work first TODO
              #
              #operator: {replicas: 2}
              operator: {replicas: 1}

              # enable full Layer 7 feature set
              envoy: {enabled: true}
              loadBalancer: {l7: {backend: envoy}}

              # features that use layer 7 functionality (envoy)
              gatewayAPI: {enabled: true}
              egressGateway: {enabled: true}
              ingressController:  # pending status until MetalLB is active
                enabled: true
                default: true
                loadbalancerMode: dedicated

              # directly inject kernel routes on each node to reach all
              # cluster pods, while masquerading extranode traffic.  only
              # possible because our cluster nodes all share a subnet.  TODO
              # does this make clustermesh impossible, have to use vxlan?
              #
              routingMode: native
              autoDirectNodeRoutes: true
              directRoutingSkipUnreachable: true
              ipv4NativeRoutingCIDR: '{{podnet}}'  # PodCIDR, see issue 12186
              bpf: {masquerade: true}

              # we'll use the built-in cilium LoadBalancer for now, consider
              # MetalLB later, it should be unnecessary as cilium can perform
              # this function instead with its l2announcements helm key and
              # CiliumLoadBalancerIPPool/CiliumL2AnnouncementPolicy resources
              #
              l2announcements: {enabled: true}

              # during bootstrap, disable hubble, it requires dataplane pods
              # to be online, which they are not during bootstrap.  we'll
              # helm-upgrade later.  we could also add a toleration to let it
              # run on control-plane tainted nodes.  without one of these,
              # cilium status --wait would hang indefinitely.
              #
              hubble:
                relay: {enabled: false}
                ui: {enabled: false}
              #
              #hubble:
              #  relay: {enabled: true}
              #  ui: {enabled: true, frontend: {server: {ipv6: {enabled: false}}}}

              # TODO later
              #clustermesh:
              #  useAPIServer: true
              #  config:
              #    domain: cluster.local
              #    clusters:
              #      - name: cluster2
              #        secretName: cilium-clustermesh

        # TODO is there a way we could use k8s.wait for this? Node=Ready does
        # not work, tried it... gets mostly there, but cilium not fully up
        #
        - name: wait_for_cni_up
          command: cilium --kubeconfig {{kubeconfig}} status --wait
          changed_when: false

    - name: wait_for_knode_readiness
      when: is_knode
      vars:
        ansible_python_interpreter: "{{py3bin_setup}}"
      delegate_to: localhost
      kubernetes.core.k8s:
        kubeconfig: '{{kubeconfig}}'
        name: "{{nodename}}.{{domain}}"
        kind: Node
        wait: true
        wait_condition: {type: Ready}
