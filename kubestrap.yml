#
---
# master: nodename=verniplex1 kubemaster=verniplex1 plexhost=vernius
# slave: nodename=verniplex2 kubemaster=null plexhost=vernius

- name: prepare_kube
  hosts: '{{nodename}}'
  vars_files:
    - vars/networks.yml
  vars:
    plexname: "{{kubemaster.name | regex_findall('^[^\\d]+') | first}}"
    kubeconfig: "{{lookup('env', 'HOME') + '/.kube/' + plexname + '.yml'}}"
    init_join_timeout: 120

    # n = third octet from the plex net's /22 base address
    # pod cidr = 10.(n).0.0/16
    # service cidr = 10.(n+1).0.0/16
    # todo: this logic is duplicated in cloudinit.tftpl
    #
    plexnet: "{{nets_plex[plexname]}}"
    octet3: "{{(plexnet | split('.'))[2] | int}}"
    podnet: "{{'10.' + (octet3 | string) + '.0.0/16'}}"
    srvnet: "{{'10.' + ((octet3 + 1) | string) + '.0.0/16'}}"

  tasks:

    # smaller number of simultaneous node spawns don't need this but 15 did
    #- name: wait_for_ssh
    #  wait_for:
    #    host: '{{kubemaster}}'
    #    port: '{{ssh_port}}'
    #    timeout: '{{ssh_open_timeout}}'
    #    search_regex: SSH
    #  delegate_to: '{{plexhost}}'

    # allow kubeadm init/join to finish
    - name: wait_for_cloudinit_completion
      command: timeout {{init_join_timeout}} cloud-init status --wait
      changed_when: false

    - name: copy_master_kubeconfig_to_controller
      synchronize:
        mode: pull
        perms: false
        dest: "{{kubeconfig}}"
        src: /etc/kubernetes/admin.conf
        use_ssh_args: true
      become: true
      when: kubemaster is not none

    # normally controlled nodes use ~config/venv/bin/python3, but
    # kubernetes installation tasks will run on the controller itself as
    # the setup user, needing extra python libraries installed from
    # requirements.txt.  so we directly specify the python interpreter
    # as the one that setup would execute by PATH lookup.  we could use
    # ~/bin/python3 as interpreter_python, but this has implications for
    # different remote_user and become_user situations, so we'll use a
    # lookup based on setup user PATH for the few cases we need this.
    #
    - name: determine_local_python3_interpreter
      command: which python3
      delegate_to: localhost
      failed_when: r.rc != 0 or 'python3' not in r.stdout
      changed_when: false
      register: r

    - name: set_ansible_python_interpreter_fact
      changed_when: false
      delegate_to: localhost
      delegate_facts: true
      set_fact:
        py3bin: "{{r.stdout}}"

    - name: kubemaster_tasks
      when: kubemaster is not none
      become: false
      delegate_to: localhost
      vars:
        ansible_python_interpreter: "{{hostvars.localhost.py3bin}}"
      block:

        # make symlinks to ~setup/.kube/*.yml work for setup group
        - name: make_kubeconfig_group_readwrite
          file:
            mode: '0660' # keep as string to avoid bugs
            path: "{{kubeconfig}}"

        - name: helm_install_cilium_repo
          kubernetes.core.helm_repository:
            #atomic: true
            name: cilium
            repo_url: https://helm.cilium.io/
            kubeconfig: "{{kubeconfig}}"

        - name: helm_install_cilium
          kubernetes.core.helm:
            #atomic: true
            kubeconfig: "{{kubeconfig}}"
            release_name: cilium
            chart_ref: cilium/cilium
            release_namespace: kube-system
            release_values:

              # in future, we might use clustermesh, so these should be unique
              cluster:
                name: "{{plexname}}"
                id: "{{octet3}}"

              # explicitly configure access to kube control plane via local
              k8sServiceHost: '{{ansible_default_ipv4.address}}'
              k8sServicePort: 6443

              # we have used skipPhases kube-proxy in ClusterConfiguration, so
              # network doesn't work until cilium is installed.
              #
              kubeProxyReplacement: true

              # this can be inferred from podSubnet, but the auto-detection is
              # inappropriate for advanced scenarios like multiple subnets.
              # since we already have the information, we'll just use it here.
              #
              ipam: {operator: {clusterPoolIPv4PodCIDRList: "{{podnet}}"}}

              # even though we run everything on one physical system, in
              # theory we can survive failed cilium pod upgrades better if we
              # have a standby replica.  todo: this should be researched more.
              # UPDATE: using 1 for now, let's make it work first TODO
              #
              #operator: {replicas: 2}
              operator: {replicas: 1}

              # enable full Layer 7 feature set
              envoy: {enabled: true}
              loadBalancer: {l7: {backend: envoy}}

              # features that use layer 7 functionality (envoy)
              gatewayAPI: {enabled: true}
              egressGateway: {enabled: true}
              ingressController:  # pending status until MetalLB is active
                enabled: true
                default: true
                loadbalancerMode: dedicated

              # directly inject kernel routes on each node to reach all
              # cluster pods, while masquerading extranode traffic.  only
              # possible because our cluster nodes all share a subnet.  TODO
              # does this make clustermesh impossible, have to use vxlan?
              #
              routingMode: native
              autoDirectNodeRoutes: true
              directRoutingSkipUnreachable: true
              ipv4NativeRoutingCIDR: '{{podnet}}'  # PodCIDR, see issue 12186
              bpf: {masquerade: true}

              # we'll use the built-in cilium LoadBalancer for now, consider
              # MetalLB later, it should be unnecessary as cilium can perform
              # this function instead with its l2announcements helm key and
              # CiliumLoadBalancerIPPool/CiliumL2AnnouncementPolicy resources
              #
              l2announcements: {enabled: true}

              # during bootstrap, disable hubble, it requires dataplane pods
              # to be online, which they are not during bootstrap.  we'll
              # helm-upgrade later.  we could also add a toleration to let it
              # run on control-plane tainted nodes.  without one of these,
              # cilium status --wait would hang indefinitely.
              #
              hubble:
                relay: {enabled: false}
                ui: {enabled: false}
              #
              #hubble:
              #  relay: {enabled: true}
              #  ui: {enabled: true, frontend: {server: {ipv6: {enabled: false}}}}

              # TODO later
              #clustermesh:
              #  useAPIServer: true
              #  config:
              #    domain: cluster.local
              #    clusters:
              #      - name: cluster2
              #        secretName: cilium-clustermesh

        - name: wait_for_cni_up
          command: cilium --kubeconfig {{kubeconfig}} status --wait
          changed_when: false
